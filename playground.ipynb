{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Franken-Llama\n",
    "\n",
    "This project aims to analyze the architecture of current SoTA transformer models, in particular Llama, by examining how the output and feature maps change by removing and/or repeating certain attention blocks of the model.\n",
    "\n",
    "Ultimately, we want to explore ways of enhancing their efficiency for potential deployment on micro-controller units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: ask hugging face for llama2 access first!\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import importlib # debugging purposes\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to import the custom libraries made specifically for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_utils\n",
    "import modified_llama\n",
    "import visualizer\n",
    "importlib.reload(visualizer)\n",
    "\n",
    "visualizer.set_display_mode(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLama2\n",
    "LLaMA 2 (Large Language Model Meta AI 2) is Meta's open-source family of advanced language models designed for tasks like text generation, summarization, and question answering. \n",
    "\n",
    "It improves upon the original LLaMA with enhanced training stability, fine-tuning, and safety features, making it more effective and accessible for research and commercial use. Available in different sizes, LLaMA 2 is optimized for both efficiency and scalability, allowing deployment on various hardware configurations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Llama2\n",
    "Just for fun, let's briefly test Llama2 capabilities by asking it to write a continuation to the string \"Once upon a time\".\n",
    "\n",
    "We'll also retrieve both the attention maps and the hidden states, which we'll use later in the visualization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "import torch\n",
    "from transformers.modeling_outputs import CausalLMOutput\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, attn_implementation=\"eager\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "model.config.output_attentions = True\n",
    "model.config.return_dict = True\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=50, output_attentions=True, return_dict_in_generate=True, output_hidden_states=True)\n",
    "\n",
    "generated_ids = outputs.sequences\n",
    "attention_weights = outputs.attentions\n",
    "states = outputs.hidden_states\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca9fea3e9b34a08955e4dde51a461fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ModifiedLlamaForCausalLM(\n",
       "  (model): ModifiedLlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import modified_llama\n",
    "mod_model = modified_llama.ModifiedLlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, attn_implementation=\"eager\")\n",
    "mod_model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama2 has 32 decode layers, each one of them representing a block in a transformer decoder.\n",
    "\n",
    "Each LlamaDecoderLayer is made of an Attention block, a MLP and 2 RMSNorms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing attention maps\n",
    "#### Layer 1\n",
    "We're first going to visualize the first layer attention map, and we'll go deeper.\n",
    "We're also just going to visualize the first head attention map, and as such we can expect the first token to get more focus compared to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 0\n",
    "head_idx = 0\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "visualizer.plot_attention_map(outputs.attentions, tokens, layer_idx=layer_idx, head_idx=head_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first layer, we can see how the model outputs a `SEQ_LENxSEQ_LEN` attention map. It's interesting how some the last tokens have some importance w.r.t to themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 20\n",
    "head_idx = 0\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "visualizer.plot_attention_map(outputs.attentions, tokens, layer_idx=layer_idx, head_idx=head_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the tokens later becomes much more difficult, as the attention is computed only w.r.t. the final token of the generated sequence.\n",
    "\n",
    "This is because Llama2, just like many other decoder only transformers, uses Masked Causal attention, which generates attention only for the last token in order to speed up computation.\n",
    "\n",
    "Going deeper into the net, we can see how the attention map increases with a value of `layer_depth + seq_len`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...finally, Franken-LLama!\n",
    "There are many ways to achieve the goal of running Llama2 without some running some specific layers (or repeating the same layer multiple times).\n",
    "\n",
    "For this reason, I've developed two ways of achieving our goal, which vary in complexity/attainment to the original Llama2 specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the from-the-ground-up classes\n",
    "\n",
    "For the scope of this project, I've decided to develop 2 classes which allow us to use a single attention layer without doing the background operations that Llama2 does in its backend (such as computing a rotary embedding layer).\n",
    "\n",
    "For this reason, the quality of the output is much lower, however the computation is faster. In addition, the attention maps cannot be analyzed. Nevertheless, it is a cool experiment and for that I think it's worthwhile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\APAI-pw\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\Desktop\\APAI-pw\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import llama_utils\n",
    "importlib.reload(llama_utils)\n",
    "test_model = llama_utils.SimpleLlamaSkipRepeat(mod_model, target_layers_idx=[], num_repeats=1, config=mod_model.config, skip_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "c:\\Users\\ASUS\\Desktop\\APAI-pw\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on <attribute 'type' of 'torch.device' objects>. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('<attribute 'type' of 'torch.device' objects>') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time ...\n",
      "\n",
      "\n",
      "\n",
      " Unterscheidung ... upon a time ... upon ... upon ...\n"
     ]
    }
   ],
   "source": [
    "output_res = test_model.generate(input_ids, max_length=20)\n",
    "generated_ids = output_res.sequences\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation is really, really poor, even without skipping any layer. This proves that the operations between each layer repetition are vital for a proper LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_res = test_model(input_ids, test_output=True)\n",
    "token_ids = output_res.squeeze().tolist()\n",
    "generated_text = tokenizer.decode(token_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   13,  2501,   263, 23196,  2023]], device='cuda:0')\n",
      "\n",
      " upon a nobody ...\n"
     ]
    }
   ],
   "source": [
    "print(output_res)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the phrase with each layer many times, it is interesting to see how the some of the semantic value of each word is somewhat retained, even after \"digesting\" the input multiple times.\n",
    "\n",
    "However, using the same layer more than 5 times results in an \"overdigested\" output, that completely loses any significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the modified transformer classes\n",
    "\n",
    "Like I mentioned, many of the operations required for Llama to work properly have been skipped in the previous version of the classes. The modified transformer classes represent an attempt to circumvent this issues by taking the source code of Llama2 and modifying it to skip or repeat some layers.\n",
    "\n",
    "> :WARNING: In order to run these models properly, it is very much suggested to reload the python kernel to free up memory. For this reason, all the imports will be repeated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "del test_model\n",
    "import modified_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_model = modified_llama.ModifiedLlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, attn_implementation=\"eager\")\n",
    "mod_model.to('cuda')\n",
    "\n",
    "importlib.reload(modified_llama)\n",
    "\n",
    "mod_model.config.output_attentions = True\n",
    "mod_model.config.return_dict = True\n",
    "mod_model.config.output_hidden_states = True\n",
    "\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "outputs = mod_model.generate(input_ids, max_length=50, output_attentions=True, return_dict_in_generate=True, output_hidden_states=True)\n",
    "\n",
    "generated_ids = outputs.sequences\n",
    "attention_weights = outputs.attentions\n",
    "states = outputs.hidden_states\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama Extraction of attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
